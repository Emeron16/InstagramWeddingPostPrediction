{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e00dd241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 1 posts.\n",
      "Scraped 2 posts.\n",
      "Scraped 3 posts.\n",
      "Scraped 4 posts.\n",
      "Scraped 5 posts.\n",
      "Data collection complete. File saved as 'wedding_posts_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "HASHTAG = \"wedding\"\n",
    "POST_LIMIT = 5  # Set the limit for how many posts to scrape\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Login (if required)\n",
    "try:\n",
    "    driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "    username_input = driver.find_element(By.NAME, 'username')\n",
    "    password_input = driver.find_element(By.NAME, 'password')\n",
    "    \n",
    "    # Replace with your credentials\n",
    "    username_input.send_keys('')\n",
    "    password_input.send_keys('')\n",
    "    \n",
    "    login_button = driver.find_element(By.XPATH, '//button[@type=\"submit\"]')\n",
    "    login_button.click()\n",
    "    time.sleep(20)\n",
    "    \n",
    "    # Navigate to the hashtag page after login\n",
    "    driver.get(f\"https://www.instagram.com/explore/tags/{HASHTAG}/\")\n",
    "    time.sleep(20)  # Allow time for the page to load\n",
    "except Exception as e:\n",
    "    print(\"Login step encountered an issue:\", e)\n",
    "\n",
    "# Scroll and scrape posts\n",
    "posts_data = []\n",
    "scraped_posts = 0\n",
    "\n",
    "while scraped_posts < POST_LIMIT:\n",
    "    # Scroll to load more posts\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(4)  # Adjust delay as needed\n",
    "    \n",
    "    # Parse the page source\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    # Find all posts on the page\n",
    "    posts = soup.find_all('a', href=True)\n",
    "    \n",
    "    for post in posts:\n",
    "        if scraped_posts >= POST_LIMIT:\n",
    "            break  # Stop if the limit is reached\n",
    "        \n",
    "        # Extract post link\n",
    "        post_url = \"https://www.instagram.com\" + post['href']\n",
    "        \n",
    "        # Ensure the URL is a post URL (skip other links)\n",
    "        if \"/p/\" not in post_url:\n",
    "            continue\n",
    "        \n",
    "        # Visit the post page to scrape data\n",
    "        driver.get(post_url)\n",
    "        time.sleep(3)  # Allow the post page to load\n",
    "        \n",
    "        # Parse the post page\n",
    "        post_html = driver.page_source\n",
    "        post_soup = BeautifulSoup(post_html, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            # Extract caption\n",
    "            caption = \"No caption\"\n",
    "            caption_element = post_soup.find('div', class_='_a9zs')\n",
    "            if caption_element:\n",
    "                caption = caption_element.text.strip()\n",
    "            \n",
    "            # Determine media type\n",
    "            media_type = \"Image\" if post_soup.find('img') else \"Video\"\n",
    "            \n",
    "            # Extract timestamp\n",
    "            timestamp = \"No timestamp\"\n",
    "            timestamp_element = post_soup.find('time')\n",
    "            if timestamp_element:\n",
    "                timestamp = timestamp_element['datetime']\n",
    "            \n",
    "            # Extract likes (if available)\n",
    "            likes = \"No likes\"\n",
    "            likes_element = post_soup.find('div', {'class': '_aacl'})\n",
    "            if likes_element:\n",
    "                likes = likes_element.text.strip()\n",
    "            \n",
    "            # Extract comments count\n",
    "            comments = \"No comments\"\n",
    "            comments_elements = post_soup.find_all('ul', {'class': '_aacl'})\n",
    "            if comments_elements:\n",
    "                comments = len(comments_elements)\n",
    "            \n",
    "            # Compile post data\n",
    "            post_data = {\n",
    "                \"Post ID\": post_url.split('/')[-2],\n",
    "                \"Caption\": caption,\n",
    "                \"Media Type\": media_type,\n",
    "                \"Timestamp\": timestamp,\n",
    "                \"Likes\": likes,\n",
    "                \"Comments\": comments\n",
    "            }\n",
    "            posts_data.append(post_data)\n",
    "            scraped_posts += 1\n",
    "            print(f\"Scraped {scraped_posts} posts.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping post: {post_url}, Error: {e}\")\n",
    "\n",
    "# Save scraped data to a CSV file\n",
    "df = pd.DataFrame(posts_data)\n",
    "df.to_csv(\"wedding_posts_data.csv\", index=False)\n",
    "print(\"Data collection complete. File saved as 'wedding_posts_data.csv'.\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d829149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data collection complete. File saved as 'wedding_posts_data.csv'.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration\n",
    "HASHTAG = \"wedding\"\n",
    "POST_LIMIT = 5  # Set the limit for how many posts to scrape\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "\n",
    "# Login (if required)\n",
    "try:\n",
    "    driver.get(\"https://www.instagram.com/accounts/login/\")\n",
    "    time.sleep(2)\n",
    "    username_input = driver.find_element(By.NAME, 'username')\n",
    "    password_input = driver.find_element(By.NAME, 'password')\n",
    "    \n",
    "    # Replace with your credentials\n",
    "    username_input.send_keys(')\n",
    "    password_input.send_keys('')\n",
    "    \n",
    "    login_button = driver.find_element(By.XPATH, '//button[@type=\"submit\"]')\n",
    "    login_button.click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Navigate to the hashtag page after login\n",
    "    driver.get(f\"https://www.instagram.com/explore/tags/{HASHTAG}/\")\n",
    "    time.sleep(5)  # Allow time for the page to load\n",
    "except Exception as e:\n",
    "    print(\"Login step encountered an issue:\", e)\n",
    "    driver.quit()\n",
    "    exit()\n",
    "\n",
    "# Scroll and load more posts\n",
    "# This part needs refinement based on Instagram's current structure\n",
    "for _ in range(5):  # Adjust the number of scroll attempts as needed\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(2)  # Allow time for posts to load\n",
    "\n",
    "# Parse the page source\n",
    "html = driver.page_source\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# Find post elements (adjust CSS selectors as needed)\n",
    "# This is a simplified example, you might need to adjust based on Instagram's HTML\n",
    "posts = soup.find_all('div', class_='_aacl')  # Example: Find elements with class _aacl (adjust as needed)\n",
    "\n",
    "posts_data = []\n",
    "scraped_posts = 0\n",
    "\n",
    "for post in posts:\n",
    "    if scraped_posts >= POST_LIMIT:\n",
    "        break\n",
    "\n",
    "    try:\n",
    "        # Extract data directly from post element (if possible)\n",
    "        # ... (Extract post_url, caption, likes, comments, timestamp, media_type as before) \n",
    "        # ... (Adjust CSS selectors within each find() method)\n",
    "\n",
    "        post_data = {\n",
    "            \"Post ID\": \"Not implemented\",  # Extract Post ID\n",
    "            \"Caption\": caption, \n",
    "            \"Media Type\": media_type,\n",
    "            \"Timestamp\": timestamp,\n",
    "            \"Likes\": likes,\n",
    "            \"Comments\": comments\n",
    "        }\n",
    "        posts_data.append(post_data)\n",
    "        scraped_posts += 1\n",
    "        print(f\"Scraped {scraped_posts} posts.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping post: {e}\")\n",
    "\n",
    "# Save scraped data to a CSV file\n",
    "df = pd.DataFrame(posts_data)\n",
    "df.to_csv(\"wedding_posts_data.csv\", index=False)\n",
    "print(\"Data collection complete. File saved as 'wedding_posts_data.csv'.\")\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5bcee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
